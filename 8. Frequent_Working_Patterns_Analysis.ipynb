{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Working Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "uri = os.getenv('NEO4J_URI')\n",
    "username = os.getenv('NEO4J_USER')\n",
    "password = os.getenv('NEO4J_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/neo4j/_sync/driver.py:493: DeprecationWarning: Relying on Driver's destructor to close the session is deprecated. Please make sure to close the session. Use it as a context (`with` statement) or make sure to call `.close()` explicitly. Future versions of the driver will not close drivers automatically.\n",
      "  deprecation_warn(\n"
     ]
    }
   ],
   "source": [
    "driver = GraphDatabase.driver(uri, auth=(username, password))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Level Event Log Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/neo4j/_sync/driver.py:493: DeprecationWarning: Relying on Driver's destructor to close the session is deprecated. Please make sure to close the session. Use it as a context (`with` statement) or make sure to call `.close()` explicitly. Future versions of the driver will not close drivers automatically.\n",
      "  deprecation_warn(\n"
     ]
    }
   ],
   "source": [
    "cypher_all_data = \"\"\"\n",
    "    MATCH (k:Kit)<-[:CORR]-(e:Event)-[:CORR]->(hbl:HighLevelBatch)\n",
    "    \n",
    "    WITH \n",
    "    date(e.timestamp) as date,\n",
    "    e.timestamp as event_timestamp,\n",
    "    e.activity as event_activity,\n",
    "    e.batch as event_batch,\n",
    "    k.kitId as kitId, \n",
    "    ID(hbl) as hbl_id,\n",
    "    hbl.date as hbl_date,\n",
    "    hbl.activity_name as hbl_activity,\n",
    "    hbl.sysId as resource,\n",
    "    hbl.corr_batch_numbers as hbl_corr_batch_numbers,\n",
    "    hbl.workTogether as work_together\n",
    "    RETURN date, event_timestamp, event_activity, event_batch, kitId, hbl_id, hbl_date, hbl_activity, resource, hbl_corr_batch_numbers, work_together\n",
    "\n",
    "    ORDER BY date,event_timestamp, resource \n",
    "\"\"\"\n",
    "\n",
    "def cypher_all_data_func(uri, username, password, cypher_all_data):\n",
    "    driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "    with driver.session() as session:\n",
    "        result = session.run(cypher_all_data)\n",
    "        data = result.data()\n",
    "        return data\n",
    "    \n",
    "results_cypher_all_data = cypher_all_data_func(uri, username, password, cypher_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_neo4j_date_to_python(neo4j_date):\n",
    "    return date(\n",
    "        year=neo4j_date.year,\n",
    "        month=neo4j_date.month,\n",
    "        day=neo4j_date.day\n",
    "    )\n",
    "\n",
    "def convert_neo4j_datetime_to_python(neo4j_datetime):\n",
    "    return datetime(\n",
    "        year=neo4j_datetime.year,\n",
    "        month=neo4j_datetime.month,\n",
    "        day=neo4j_datetime.day,\n",
    "        hour=neo4j_datetime.hour,\n",
    "        minute=neo4j_datetime.minute,\n",
    "        second=neo4j_datetime.second,\n",
    "        microsecond=neo4j_datetime.nanosecond // 1000, \n",
    "        tzinfo=neo4j_datetime.tzinfo\n",
    "    )\n",
    "df = pd.DataFrame(results_cypher_all_data)\n",
    "\n",
    "df['date'] = df['date'].apply(convert_neo4j_date_to_python)\n",
    "df['date'] = df['date'].apply(lambda d: d.strftime('%Y-%m-%d'))\n",
    "\n",
    "df['hbl_date'] = df['hbl_date'].apply(convert_neo4j_date_to_python)\n",
    "df['hbl_date'] = df['hbl_date'].apply(lambda d: d.strftime('%Y-%m-%d'))\n",
    "\n",
    "df['event_timestamp'] = df['event_timestamp'].apply(convert_neo4j_datetime_to_python)\n",
    "\n",
    "df['resource'] = df['resource'].astype(str)\n",
    "df['work_together'] = df['work_together'].astype(str)\n",
    "\n",
    "df['hbl_activity'] = df['hbl_activity'].astype(str)\n",
    "df['hbl_activity'] = df['hbl_activity'].str.strip('[]')\n",
    "df['hbl_activity'] = df['hbl_activity'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\"))\n",
    "\n",
    "df['hbl_corr_batch_numbers'] = df['hbl_corr_batch_numbers'].astype(str)\n",
    "df['hbl_corr_batch_numbers'] = df['hbl_corr_batch_numbers'].str.strip('[]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['date', 'resource', 'event_timestamp'], ascending=True)\n",
    "\n",
    "hbl_global_id = 0\n",
    "previous_hbl_id = None\n",
    "\n",
    "def assign_hbl_global_id(row):\n",
    "    global hbl_global_id, previous_hbl_id\n",
    "    if row['hbl_id'] != previous_hbl_id:\n",
    "        hbl_global_id += 1\n",
    "        previous_hbl_id = row['hbl_id']\n",
    "    return hbl_global_id\n",
    "\n",
    "df['hbl_global_id'] = df.apply(assign_hbl_global_id, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('hbl_global_id')\n",
    "\n",
    "hlb_df = grouped.agg(\n",
    "    hbl_date=('hbl_date', 'first'),\n",
    "    hbl_activity=('hbl_activity', 'first'),\n",
    "    resource=('resource', 'first'),\n",
    "    earliest_timestamp=('event_timestamp', 'min'),\n",
    "    latest_timestamp=('event_timestamp', 'max'),\n",
    "    event_number=('event_timestamp', 'count'),\n",
    "    kits=('kitId', 'nunique'),\n",
    "    batch_instance_number=('event_batch', 'nunique'),\n",
    "    batch_instances=('event_batch', lambda x: ', '.join(map(str, x.unique()))),\n",
    "    hbl_id=('hbl_id', 'first'),\n",
    "    work_together=('work_together', 'first')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlb_df['hbl_date'] = pd.to_datetime(hlb_df['hbl_date'])\n",
    "hlb_df['earliest_timestamp'] = pd.to_datetime(hlb_df['earliest_timestamp'])\n",
    "hlb_df['latest_timestamp'] = pd.to_datetime(hlb_df['latest_timestamp'])\n",
    "\n",
    "hlb_df['month'] = hlb_df['hbl_date'].dt.month\n",
    "hlb_df['day_of_week'] = hlb_df['hbl_date'].dt.dayofweek\n",
    "\n",
    "hlb_df = hlb_df.sort_values(by=['hbl_date', 'resource', 'hbl_global_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_shift(timestamp):\n",
    "    if timestamp.time() >= pd.Timestamp('00:00:00').time() and timestamp.time() < pd.Timestamp('09:30:00').time():\n",
    "        return '1'\n",
    "    elif timestamp.time() >= pd.Timestamp('09:30:00').time() and timestamp.time() < pd.Timestamp('11:30:00').time():\n",
    "        return '2'\n",
    "    elif timestamp.time() >= pd.Timestamp('11:30:00').time() and timestamp.time() < pd.Timestamp('14:00:00').time():\n",
    "        return '3'\n",
    "    else:\n",
    "        return '4'\n",
    "\n",
    "hlb_df['shift'] = hlb_df.groupby(['resource', 'hbl_date'])['earliest_timestamp'].transform(lambda x: assign_shift(x.min()))\n",
    "hlb_df['shift'] = hlb_df['shift'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_weekdays = hlb_df[hlb_df['day_of_week'] < 5]\n",
    "final_df_weekends = hlb_df[hlb_df['day_of_week'] > 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Patterns Weekdays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1r/3fk_x7p96299qsbl_p0t7v6h0000gn/T/ipykernel_14691/3721520294.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df_weekdays['earliest_timestamp'] = pd.to_datetime(final_df_weekdays['earliest_timestamp'])\n",
      "/var/folders/1r/3fk_x7p96299qsbl_p0t7v6h0000gn/T/ipykernel_14691/3721520294.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df_weekdays['latest_timestamp'] = pd.to_datetime(final_df_weekdays['latest_timestamp'])\n",
      "/var/folders/1r/3fk_x7p96299qsbl_p0t7v6h0000gn/T/ipykernel_14691/3721520294.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df_weekdays['work_together'] = final_df_weekdays['work_together'].astype(str)\n",
      "/var/folders/1r/3fk_x7p96299qsbl_p0t7v6h0000gn/T/ipykernel_14691/3721520294.py:23: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  df_encoded = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n"
     ]
    }
   ],
   "source": [
    "final_df_weekdays['earliest_timestamp'] = pd.to_datetime(final_df_weekdays['earliest_timestamp'])\n",
    "final_df_weekdays['latest_timestamp'] = pd.to_datetime(final_df_weekdays['latest_timestamp'])\n",
    "final_df_weekdays['work_together'] = final_df_weekdays['work_together'].astype(str)\n",
    "\n",
    "def extract_sequences(df):\n",
    "    sequences = defaultdict(list)\n",
    "    for _, group in df.groupby(['resource', 'day_of_week', 'shift', 'hbl_date']):\n",
    "        seq = []\n",
    "        for _, row in group.sort_values('earliest_timestamp').iterrows():\n",
    "            activity = f\"{row['hbl_activity']} ({'Together' if row['work_together'] == 'True' else 'Separate'})\"\n",
    "            seq.append(activity)\n",
    "        if len(seq) > 1: \n",
    "            sequences[(group['resource'].iloc[0], group['day_of_week'].iloc[0], group['shift'].iloc[0], group['hbl_date'].iloc[0])] = seq\n",
    "    return sequences\n",
    "\n",
    "sequences = extract_sequences(final_df_weekdays)\n",
    "\n",
    "flat_sequences = list(sequences.values())\n",
    "\n",
    "# Apply TransactionEncoder to transform the sequences for pattern mining\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(flat_sequences).transform(flat_sequences, sparse=True)\n",
    "df_encoded = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply the fpgrowth algorithm to find frequent patterns\n",
    "frequent_patterns = fpgrowth(df_encoded, min_support=0.1, use_colnames=True)\n",
    "\n",
    "frequent_patterns['sequence'] = frequent_patterns['itemsets'].apply(lambda x: ' -> '.join(list(x)))\n",
    "frequent_patterns = frequent_patterns.drop(columns=['itemsets'])\n",
    "\n",
    "context_records = []\n",
    "for (resource, day_of_week, shift, hbl_date), seq in sequences.items():\n",
    "    context_records.append({\n",
    "        'resource': resource, \n",
    "        'day_of_week': day_of_week, \n",
    "        'shift': shift, \n",
    "        'hbl_date': hbl_date,\n",
    "        'sequence': seq,\n",
    "        'sequence_str': ' -> '.join(seq)\n",
    "    })\n",
    "context_df = pd.DataFrame(context_records)\n",
    "\n",
    "total_sequences = context_df.groupby(['day_of_week', 'shift']).size().reset_index(name='Traces on a weekday and shift')\n",
    "\n",
    "def match_patterns(patterns_df, context_df):\n",
    "    matched_patterns = []\n",
    "    for _, pattern_row in patterns_df.iterrows():\n",
    "        pattern = pattern_row['sequence']\n",
    "        pattern_parts = pattern.split(' -> ')\n",
    "        for _, context_row in context_df.iterrows():\n",
    "            context_sequence = context_row['sequence']\n",
    "            context_sequence_str = context_row['sequence_str']\n",
    "            if all(item in context_sequence_str for item in pattern_parts):\n",
    "                pattern_work_together = [part.split('(')[1].strip(')') for part in pattern_parts]\n",
    "                context_work_together = [activity.split('(')[1].strip(')') for activity in context_sequence if any(activity.startswith(part.split(' (')[0]) for part in pattern_parts)]\n",
    "                if pattern_work_together == context_work_together:\n",
    "                    matched_patterns.append({\n",
    "                        'day_of_week': context_row['day_of_week'],\n",
    "                        'shift': context_row['shift'],\n",
    "                        'sequence': pattern,\n",
    "                        'resource': context_row['resource'],\n",
    "                        'hbl_date': context_row['hbl_date']\n",
    "                    })\n",
    "    return pd.DataFrame(matched_patterns)\n",
    "\n",
    "\n",
    "matched_patterns_df = match_patterns(frequent_patterns, context_df)\n",
    "\n",
    "matched_patterns_df_unique = matched_patterns_df.drop_duplicates(subset=['day_of_week', 'shift', 'sequence', 'resource', 'hbl_date'])\n",
    "\n",
    "pattern_counts = matched_patterns_df_unique.groupby(['day_of_week', 'shift', 'sequence']).size().reset_index(name='Traces with a pattern')\n",
    "\n",
    "pattern_counts = pattern_counts.merge(total_sequences, on=['day_of_week', 'shift'])\n",
    "pattern_counts['Percentage'] = (pattern_counts['Traces with a pattern'] / pattern_counts['Traces on a weekday and shift']) * 100\n",
    "\n",
    "day_mapping = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "pattern_counts['Day name'] = pattern_counts['day_of_week'].map(day_mapping)\n",
    "\n",
    "shift_order = [1, 2, 3, 4]\n",
    "pattern_counts['shift'] = pd.Categorical(pattern_counts['shift'], categories=shift_order, ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Patterns Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1r/3fk_x7p96299qsbl_p0t7v6h0000gn/T/ipykernel_14691/778533589.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df_weekends['earliest_timestamp'] = pd.to_datetime(final_df_weekends['earliest_timestamp'])\n",
      "/var/folders/1r/3fk_x7p96299qsbl_p0t7v6h0000gn/T/ipykernel_14691/778533589.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df_weekends['latest_timestamp'] = pd.to_datetime(final_df_weekends['latest_timestamp'])\n",
      "/var/folders/1r/3fk_x7p96299qsbl_p0t7v6h0000gn/T/ipykernel_14691/778533589.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df_weekends['work_together'] = final_df_weekends['work_together'].astype(str)\n",
      "/var/folders/1r/3fk_x7p96299qsbl_p0t7v6h0000gn/T/ipykernel_14691/778533589.py:23: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  df_encoded = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n"
     ]
    }
   ],
   "source": [
    "final_df_weekends['earliest_timestamp'] = pd.to_datetime(final_df_weekends['earliest_timestamp'])\n",
    "final_df_weekends['latest_timestamp'] = pd.to_datetime(final_df_weekends['latest_timestamp'])\n",
    "final_df_weekends['work_together'] = final_df_weekends['work_together'].astype(str)\n",
    "\n",
    "def extract_sequences(df):\n",
    "    sequences = defaultdict(list)\n",
    "    for _, group in df.groupby(['resource', 'day_of_week', 'shift', 'hbl_date']):\n",
    "        seq = []\n",
    "        for _, row in group.sort_values('earliest_timestamp').iterrows():\n",
    "            activity = f\"{row['hbl_activity']} ({'Together' if row['work_together'] == 'True' else 'Separate'})\"\n",
    "            seq.append(activity)\n",
    "        if len(seq) > 1: \n",
    "            sequences[(group['resource'].iloc[0], group['day_of_week'].iloc[0], group['shift'].iloc[0], group['hbl_date'].iloc[0])] = seq\n",
    "    return sequences\n",
    "\n",
    "sequences_weekends = extract_sequences(final_df_weekends)\n",
    "\n",
    "flat_sequences_weekends = list(sequences_weekends.values())\n",
    "\n",
    "# Apply TransactionEncoder to transform the sequences for pattern mining\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(flat_sequences_weekends).transform(flat_sequences_weekends, sparse=True)\n",
    "df_encoded = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply the fpgrowth algorithm to find frequent patterns\n",
    "frequent_patterns_weekends = fpgrowth(df_encoded, min_support=0.1, use_colnames=True)\n",
    "\n",
    "frequent_patterns_weekends['sequence'] = frequent_patterns_weekends['itemsets'].apply(lambda x: ' -> '.join(list(x)))\n",
    "frequent_patterns_weekends = frequent_patterns_weekends.drop(columns=['itemsets'])\n",
    "\n",
    "context_records = []\n",
    "for (resource, day_of_week, shift, hbl_date), seq in sequences_weekends.items():\n",
    "    context_records.append({\n",
    "        'resource': resource, \n",
    "        'day_of_week': day_of_week, \n",
    "        'shift': shift, \n",
    "        'hbl_date': hbl_date,\n",
    "        'sequence': seq,\n",
    "        'sequence_str': ' -> '.join(seq)\n",
    "    })\n",
    "context_df = pd.DataFrame(context_records)\n",
    "\n",
    "total_sequences_weekends = context_df.groupby(['day_of_week', 'shift']).size().reset_index(name='Traces on a weekday and shift')\n",
    "\n",
    "def match_patterns(patterns_df, context_df):\n",
    "    matched_patterns = []\n",
    "    for _, pattern_row in patterns_df.iterrows():\n",
    "        pattern = pattern_row['sequence']\n",
    "        pattern_parts = pattern.split(' -> ')\n",
    "        for _, context_row in context_df.iterrows():\n",
    "            context_sequence = context_row['sequence']\n",
    "            context_sequence_str = context_row['sequence_str']\n",
    "            if all(item in context_sequence_str for item in pattern_parts):\n",
    "                pattern_work_together = [part.split('(')[1].strip(')') for part in pattern_parts]\n",
    "                context_work_together = [activity.split('(')[1].strip(')') for activity in context_sequence if any(activity.startswith(part.split(' (')[0]) for part in pattern_parts)]\n",
    "                if pattern_work_together == context_work_together:\n",
    "                    matched_patterns.append({\n",
    "                        'day_of_week': context_row['day_of_week'],\n",
    "                        'shift': context_row['shift'],\n",
    "                        'sequence': pattern,\n",
    "                        'resource': context_row['resource'],\n",
    "                        'hbl_date': context_row['hbl_date']\n",
    "                    })\n",
    "    return pd.DataFrame(matched_patterns)\n",
    "\n",
    "matched_patterns_df_weekends = match_patterns(frequent_patterns_weekends, context_df)\n",
    "\n",
    "matched_patterns_df_unique_weekends = matched_patterns_df_weekends.drop_duplicates(subset=['day_of_week', 'shift', 'sequence', 'resource', 'hbl_date'])\n",
    "\n",
    "pattern_counts_weekends = matched_patterns_df_unique_weekends.groupby(['day_of_week', 'shift', 'sequence']).size().reset_index(name='Traces with a pattern')\n",
    "\n",
    "pattern_counts_weekends = pattern_counts_weekends.merge(total_sequences_weekends, on=['day_of_week', 'shift'])\n",
    "pattern_counts_weekends['Percentage'] = (pattern_counts_weekends['Traces with a pattern'] / pattern_counts_weekends['Traces on a weekday and shift']) * 100\n",
    "\n",
    "day_mapping = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "pattern_counts_weekends['Day name'] = pattern_counts_weekends['day_of_week'].map(day_mapping)\n",
    "\n",
    "shift_order = [1, 2, 3, 4]\n",
    "pattern_counts_weekends['shift'] = pd.Categorical(pattern_counts_weekends['shift'], categories=shift_order, ordered=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
